{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data and save it into pickle files for easier usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['05. Dec (Thursday)', '18. Dec (Wednesday)'])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filepath = 'raw_data.json'\n",
    "with open(filepath) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# test access\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut data (Code from Elias)\n",
    "def cut_data(data, cut_start, cut_end):\n",
    "    return [timestamp for timestamp in data if timestamp >= cut_start and timestamp <= cut_end]\n",
    "\n",
    "cut_start = 1734519960000 # 12:06\n",
    "cut_end = 1734520799000 # 12:19:59\n",
    "\n",
    "for spot in data[\"18. Dec (Wednesday)\"]:\n",
    "    if spot == \"Metadata\":\n",
    "        continue\n",
    "    data[\"18. Dec (Wednesday)\"][spot] = cut_data(data[\"18. Dec (Wednesday)\"][spot], cut_start, cut_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group into groups of 30 seconds\n",
    "def group_timestamps(timestamps, group_window=30, starter=None, end=None):\n",
    "    groups = []\n",
    "    # First, convert all timestamps into seconds\n",
    "    timestamps = [t // 1000 for t in timestamps]\n",
    "    # Use sorted list to make it easier\n",
    "    timestamps = sorted(timestamps) \n",
    "    if starter is None:\n",
    "        group_starter = timestamps[0]\n",
    "    else:\n",
    "        group_starter = starter // 1000\n",
    "    if end is None:\n",
    "        group_end = timestamps[-1]\n",
    "    else:\n",
    "        group_end = end // 1000\n",
    "    group = []\n",
    "    for t in timestamps:\n",
    "        if t - group_starter < group_window:\n",
    "            group.append(t)\n",
    "        else:\n",
    "            groups.append(group)\n",
    "            group_starter = group_starter + group_window\n",
    "            group = []\n",
    "            \n",
    "            while t - group_starter >= group_window:\n",
    "                groups.append([])\n",
    "                group_starter = group_starter + group_window\n",
    "\n",
    "            group.append(t)\n",
    "\n",
    "    # pad until end\n",
    "    while group_starter + group_window <= group_end:\n",
    "        groups.append([])\n",
    "        group_starter = group_starter + group_window\n",
    "\n",
    "    # Add the last group\n",
    "    groups.append(group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and save the groupings\n",
    "second_day_grouped = {}\n",
    "for spot in data[\"18. Dec (Wednesday)\"].keys():\n",
    "    if spot == \"Metadata\":\n",
    "        continue\n",
    "    second_day_grouped[spot] = group_timestamps(data[\"18. Dec (Wednesday)\"][spot], group_window=30, starter=cut_start, end=cut_end)\n",
    "\n",
    "filename = \"data/second_day_g30.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(second_day_grouped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as [timframes][spots] -> value instead of:\n",
    "#         [spots][timeframes] -> value\n",
    "filename = \"data/second_day_g30_transposed.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    data_transposed = []\n",
    "    for i in range(len(second_day_grouped[\"Auswahl\"])):\n",
    "        data_transposed.append({})\n",
    "        for spot in second_day_grouped.keys():\n",
    "            data_transposed[i][spot] = second_day_grouped[spot][i]\n",
    "\n",
    "    pickle.dump(data_transposed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keys:\n",
    "filename = \"data/second_day_keys.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(list(second_day_grouped.keys()), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataLiteracy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
