{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw_data.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting global variables:\n",
    "- where to cut data\n",
    "- what bin size to use (in ms)\n",
    "- if data should be shifted across spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_start = 1734519960000 # 12:06\n",
    "cut_end = 1734520799000 # 12:19:59\n",
    "binsize = 15000 # ms = 15s\n",
    "shift = 1\n",
    "outfile_model = \"../data-markov/probabilities-15s-shift-1.csv\"\n",
    "outfile_metrics = \"../data-markov/markov_metrics_long.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General helper functions to cut and bin the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut data\n",
    "def cut_data(data, cut_start, cut_end):\n",
    "    return [timestamp for timestamp in data if timestamp >= cut_start and timestamp <= cut_end]\n",
    "\n",
    "# count values in bins\n",
    "def bin_data(data, start, end, binsize):\n",
    "    bins = np.zeros(int((end - start) / binsize) + 1)\n",
    "    for timestamp in data:\n",
    "        bins[int((timestamp - start) / binsize)] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data by cutting to start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spot in data[\"18. Dec (Wednesday)\"]:\n",
    "    if spot == \"Metadata\":\n",
    "        continue\n",
    "    data[\"18. Dec (Wednesday)\"][spot] = cut_data(data[\"18. Dec (Wednesday)\"][spot], cut_start, cut_end)\n",
    "\n",
    "d = data[\"18. Dec (Wednesday)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for the regression and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_rate(data1, data2):\n",
    "    regr = LinearRegression().fit(data1, data2)\n",
    "    return regr.coef_[0][0]\n",
    "\n",
    "# format data for regression\n",
    "def get_regr_data(spot1, spot2, shift=0):\n",
    "    # if shift present, cut end from to_state, start from end_state so that s1(ti) -> s2(ti+shift)\n",
    "    data1 = spot1[:-shift] if shift > 0 else spot1\n",
    "    data2 = spot2[shift:]\n",
    "\n",
    "    # y is the combined vectors of both vectors\n",
    "    y = []\n",
    "    y.append(data1)\n",
    "    y.append(data2)\n",
    "    y = np.array(y)\n",
    "    y.flatten()\n",
    "    y = np.array(y).reshape(-1,1)\n",
    "\n",
    "    # x is the identifiers, 0 for the first spot, 1 for the second spot\n",
    "    x = [0] * len(data1) + [1] * len(data2)\n",
    "    x = np.array(x).reshape(-1,1)\n",
    "    return x, y\n",
    "\n",
    "# returns sum of masses at specified spots\n",
    "def metric_summed(pop, end_spots, spots):\n",
    "    positions = []\n",
    "    for end_spot in end_spots:\n",
    "        positions.append(spots.index(end_spot))\n",
    "    metric_sum = 0\n",
    "    for pos in positions:\n",
    "        metric_sum += pop[pos]\n",
    "    return metric_sum\n",
    "\n",
    "# simulate a markov model by matrix multiplication\n",
    "# can also simulate on a list of probabilities by setting type to any value except simple\n",
    "def simulate_markov(initial_pop, transition_matrix, n=1, type=\"simple\"):\n",
    "    crt_pop = initial_pop\n",
    "    for i in range(n):\n",
    "        # if only one transition matrix is given\n",
    "        if type == \"simple\":\n",
    "            crt_pop = np.dot(crt_pop, transition_matrix)\n",
    "        # if list of matrices is given (simulate over time)\n",
    "        else: \n",
    "            crt_pop = np.dot(crt_pop, transition_matrix[i])\n",
    "    return crt_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to generate a markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(d, spots, binsize, start, end, guide_dict, shift=0):\n",
    "    d_binned = {}\n",
    "    for spot, timestamps in d.items():\n",
    "        if spot not in spots:\n",
    "            continue\n",
    "        binned_data = bin_data(timestamps, start, end, binsize)\n",
    "        d_binned[spot] = binned_data\n",
    "    \n",
    "    M = np.zeros((len(spots),len(spots)))\n",
    "\n",
    "    # for each row\n",
    "    for i, spot1 in enumerate(spots):\n",
    "        # assign pass probabilities, or 0\n",
    "        for j, spot2 in enumerate(spots):\n",
    "            # if current spot is allowed\n",
    "            if guide_dict[spot1][j] != \"None\":\n",
    "                x, y = get_regr_data(d_binned[spot1], d_binned[spot2], shift=shift)\n",
    "                # negative flow rate -> queue, vice versa\n",
    "                M[i][j] = get_flow_rate(x, y)\n",
    "            else: \n",
    "                M[i][j] = - np.inf\n",
    "        \n",
    "        flowrates = [x for x in M[i] if x != - np.inf]\n",
    "        min_flow = min(flowrates)\n",
    "        max_flow = max(flowrates)\n",
    "        uniform_probability = 1 / len(flowrates)\n",
    "\n",
    "\n",
    "        # transform to [0,1] or set to 0\n",
    "        for j, spot2 in enumerate(spots):\n",
    "            if M[i][j] != - np.inf:\n",
    "                if max_flow != min_flow:\n",
    "                    # [0,1] scaling approach\n",
    "                    flow_factor = ((M[i][j] - min_flow) / (max_flow - min_flow))\n",
    "                    M[i][j] = flow_factor\n",
    "                else:\n",
    "                    M[i][j] = uniform_probability\n",
    "            else:\n",
    "                M[i][j] = 0\n",
    "        # cleanup\n",
    "        sum_of_transitions = np.sum(M[i])\n",
    "        if sum_of_transitions != 1:\n",
    "                for j in range(len(spots)):\n",
    "                    M[i][j] = M[i][j] / sum_of_transitions\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for the markov model\n",
    "- Guide dict is a dictionary containing all legal transitions\n",
    "- spots is a list of named spots in guide_dict and data to consider for the analysis\n",
    "- end spots is a subset of spots, and are considered the final spots for the metrics later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_dict = {'Entrance_R':     [\"Pass\", \"None\", \"None\", \"Pass\", \"Pass\", \"None\", \"None\", \"Pass\", \"None\", \"None\"],\n",
    "              'Entrance_L':     [\"None\", \"Pass\", \"None\", \"Pass\", \"Pass\", \"None\", \"None\", \"Pass\", \"None\", \"None\"],\n",
    "              'Cutlery':        [\"Pass\", \"Pass\", \"Pass\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\"],\n",
    "              'Auswahl':        [\"None\", \"None\", \"None\", \"Pass\", \"None\", \"Pass\", \"Pass\", \"None\", \"Pass\", \"Pass\"],\n",
    "              'Day_Menu':       [\"None\", \"None\", \"None\", \"None\", \"Pass\", \"Pass\", \"Pass\", \"None\", \"Pass\", \"Pass\"],\n",
    "              'Cash_T':         [\"None\", \"None\", \"None\", \"None\", \"None\", \"Pass\", \"None\", \"None\", \"None\", \"None\"],\n",
    "              'Cash_B':         [\"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"Pass\", \"None\", \"None\", \"None\"],\n",
    "              'Veggie':         [\"None\", \"None\", \"None\", \"None\", \"None\", \"Pass\", \"Pass\", \"Pass\", \"Pass\", \"Pass\"],\n",
    "              'Veggie_Cash_R':  [\"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"Pass\", \"None\"],\n",
    "              'Veggie_Cash_L':  [\"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"Pass\"]\n",
    "}\n",
    "\n",
    "spots = ['Entrance_R','Entrance_L','Cutlery','Auswahl','Day_Menu','Cash_T', 'Cash_B','Veggie','Veggie_Cash_R','Veggie_Cash_L']\n",
    "end_spots = ['Cash_T', 'Cash_B','Veggie_Cash_R','Veggie_Cash_L']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the markov model, and saving its probability matrix to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_model = get_regression_model(d, spots, binsize, cut_start, cut_end, guide_dict, shift)\n",
    "pd.DataFrame(regr_model).to_csv(outfile_model, header=spots, index=spots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating and saving the results to a file in long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', 'Observed', np.float64(0.13778818320313418)], ['3', 'Theoretical A', np.float64(0.19570928172358962)], ['3', 'Theoretical B', np.float64(0.39141856344717924)], ['4', 'Observed', np.float64(0.334294536450545)], ['4', 'Theoretical A', np.float64(0.4168587259600049)], ['4', 'Theoretical B', np.float64(0.6380081701964203)], ['5', 'Observed', np.float64(0.5228151776050011)], ['5', 'Theoretical A', np.float64(0.6026249189150706)], ['5', 'Theoretical B', np.float64(0.7883911118701364)], ['6', 'Observed', np.float64(0.6748860108705194)], ['6', 'Theoretical A', np.float64(0.7399490265164741)], ['6', 'Theoretical B', np.float64(0.8772731341178776)], ['7', 'Observed', np.float64(0.7862654427836943)], ['7', 'Theoretical A', np.float64(0.8344839773964768)], ['7', 'Theoretical B', np.float64(0.9290189282764798)], ['8', 'Observed', np.float64(0.8630622720317846)], ['8', 'Theoretical A', np.float64(0.8967163151609135)], ['8', 'Theoretical B', np.float64(0.9589486529253503)], ['9', 'Observed', np.float64(0.9139157646962814)], ['9', 'Theoretical A', np.float64(0.9364717975497797)], ['9', 'Theoretical B', np.float64(0.976227279938646)], ['10', 'Observed', np.float64(0.9466495644540154)], ['10', 'Theoretical A', np.float64(0.9613397761980881)], ['10', 'Theoretical B', np.float64(0.9862077548463968)]]\n"
     ]
    }
   ],
   "source": [
    "initial_pop_all = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "changed_model = regr_model.copy()\n",
    "changed_model[2] = [0.25,0.25,0.5,0,0,0,0,0,0,0]\n",
    "\n",
    "changed_model_2 = regr_model.copy()\n",
    "changed_model_2[2] = [0.5,0.5,0,0,0,0,0,0,0,0]\n",
    "\n",
    "rdf = []\n",
    "for i in range(3, 11):\n",
    "    end_pops_all = simulate_markov(initial_pop_all, regr_model, n=i)\n",
    "    rdf.append([str(i), \"Observed\", metric_summed(end_pops_all, end_spots, spots)])\n",
    "\n",
    "    end_pops_changed = simulate_markov(initial_pop_all, changed_model, n=i)\n",
    "    rdf.append([str(i), \"Theoretical A\", metric_summed(end_pops_changed, end_spots, spots)])\n",
    "\n",
    "    end_pops_changed_2 = simulate_markov(initial_pop_all, changed_model_2, n=i)\n",
    "    rdf.append([str(i), \"Theoretical B\", metric_summed(end_pops_changed_2, end_spots, spots)])\n",
    "\n",
    "print(rdf)\n",
    "\n",
    "pd.DataFrame(rdf).to_csv(outfile_metrics, header=[\"Simulations\", \"Model\", \"Metric\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
